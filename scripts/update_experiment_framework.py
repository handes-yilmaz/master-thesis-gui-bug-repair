#!/usr/bin/env python
"""
Update Experiment Framework to Use Real SWE-bench Bugs
======================================================

This script updates your experiment framework to use real SWE-bench bugs
instead of synthetic scenarios, making research more credible.
"""

import json
import shutil
from pathlib import Path
from typing import List, Dict, Any

def backup_original_scenarios():
    """Backup the original synthetic scenarios"""
    print("üíæ Backing up original synthetic scenarios...")
    
    # Backup the synthetic scenarios
    synthetic_file = Path("data/processed/advanced_scenarios.json")
    if synthetic_file.exists():
        backup_file = Path("data/processed/advanced_scenarios_synthetic_backup.json")
        shutil.copy2(synthetic_file, backup_file)
        print(f"‚úÖ Backed up to: {backup_file}")
    else:
        print("‚ö†Ô∏è No synthetic scenarios file found to backup")
    
    # Backup the enhanced experiment framework
    framework_file = Path("scripts/experiments/enhanced_experiment_framework.py")
    if framework_file.exists():
        backup_file = Path("scripts/experiments/enhanced_experiment_framework_synthetic_backup.py")
        shutil.copy2(framework_file, backup_file)
        print(f"‚úÖ Backed up to: {backup_file}")

def update_experiment_framework():
    """Update the experiment framework to use real SWE-bench bugs"""
    print("üîÑ Updating experiment framework to use real SWE-bench bugs...")
    
    # Load the converted SWE-bench bugs
    swe_bugs_file = Path("data/processed/swe_bench_experiment_bugs.json")
    if not swe_bugs_file.exists():
        print("‚ùå SWE-bench bugs file not found!")
        print("Run the integrate_swe_bench_bugs.py script first.")
        return False
    
    with open(swe_bugs_file, 'r', encoding='utf-8') as f:
        swe_bugs = json.load(f)
    
    print(f"‚úÖ Loaded {len(swe_bugs)} real SWE-bench bugs")
    
    # Replace the synthetic scenarios with real bugs
    output_file = Path("data/processed/real_swe_bench_scenarios.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(swe_bugs, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ Saved real scenarios to: {output_file}")
    
    # Update the enhanced experiment framework to use real bugs
    update_framework_code(swe_bugs)
    
    return True

def update_framework_code(swe_bugs: List[Dict[str, Any]]):
    """Update the enhanced experiment framework code to use real bugs"""
    print("üîß Updating experiment framework code...")
    
    framework_file = Path("scripts/experiments/enhanced_experiment_framework.py")
    if not framework_file.exists():
        print("‚ùå Enhanced experiment framework not found!")
        return
    
    # Read the current framework
    with open(framework_file, 'r', encoding='utf-8') as f:
        framework_code = f.read()
    
    # Update the scenarios loading to use real bugs
    old_loading_code = """        # Load advanced scenarios
        scenarios_file = Path("data/processed/advanced_scenarios.json")
        if scenarios_file.exists():
            with open(scenarios_file, 'r') as f:
                self.advanced_scenarios = json.load(f)
            print(f"‚úì Loaded {len(self.advanced_scenarios)} advanced scenarios")
        else:
            print("‚ö† No advanced scenarios found, using empty list")
            self.advanced_scenarios = []"""
    
    new_loading_code = """        # Load real SWE-bench scenarios
        scenarios_file = Path("data/processed/real_swe_bench_scenarios.json")
        if scenarios_file.exists():
            with open(scenarios_file, 'r') as f:
                self.advanced_scenarios = json.load(f)
            print(f"‚úì Loaded {len(self.advanced_scenarios)} real SWE-bench GUI bugs")
        else:
            print("‚ö† No real SWE-bench scenarios found, falling back to synthetic")
            fallback_file = Path("data/processed/advanced_scenarios.json")
            if fallback_file.exists():
                with open(fallback_file, 'r') as f:
                    self.advanced_scenarios = json.load(f)
                print(f"‚úì Loaded {len(self.advanced_scenarios)} synthetic scenarios as fallback")
            else:
                print("‚ö† No scenarios found, using empty list")
                self.advanced_scenarios = []"""
    
    # Replace the code
    if old_loading_code in framework_code:
        framework_code = framework_code.replace(old_loading_code, new_loading_code)
        
        # Write the updated framework
        with open(framework_file, 'w', encoding='utf-8') as f:
            f.write(framework_code)
        
        print(f"‚úÖ Updated experiment framework: {framework_file}")
    else:
        print("‚ö†Ô∏è Could not find the exact code to replace in framework")
        print("You may need to manually update the scenarios loading section")

def create_real_data_experiment_runner():
    """Create a new experiment runner specifically for real SWE-bench bugs"""
    print("üöÄ Creating real data experiment runner...")
    
    runner_code = """#!/usr/bin/env python
\"\"\"
Real SWE-bench GUI Bug Experiment Runner
========================================

This script runs experiments on real SWE-bench GUI bugs,
providing authentic research results.
\"\"\"

import json
from pathlib import Path
from scripts.experiments.enhanced_experiment_framework import EnhancedExperimentFramework

def run_real_bug_experiments():
    \"\"\"Run experiments on real SWE-bench GUI bugs\"\"\"
    
    print("üî¨ Real SWE-bench GUI Bug Experiments")
    print("=" * 50)
    
    # Initialize framework with real bugs
    framework = EnhancedExperimentFramework()
    
    print(f"‚úì Framework initialized with {len(framework.llm_clients)} models")
    print(f"‚úì Loaded {len(framework.advanced_scenarios)} real SWE-bench GUI bugs")
    
    # Show bug distribution
    show_bug_distribution(framework.advanced_scenarios)
    
    # Run experiments on real bugs
    all_results = []
    
    try:
        print("\\nüöÄ Starting experiments on real GUI bugs...")
        
        # Test different bug categories
        bug_categories = ['form', 'responsive', 'styling', 'ui_layout', 'interaction']
        
        for category in bug_categories:
            category_bugs = [bug for bug in framework.advanced_scenarios if bug.get('bug_category') == category]
            if category_bugs:
                print(f"\\nüìä Testing {len(category_bugs)} {category} bugs...")
                
                # Test first 3 bugs in each category to start
                test_bugs = category_bugs[:3]
                for bug in test_bugs:
                    print(f"  Testing: {bug.get('title', 'No title')[:60]}...")
                    
                    # Run experiment on this bug
                    result = framework.run_advanced_experiment(
                        bug_description=bug.get('description', ''),
                        modality='text_only',
                        prompt_template='technical_detailed',
                        bug_category=bug.get('bug_category', ''),
                        bug_severity=bug.get('severity', ''),
                        bug_difficulty=bug.get('difficulty', '')
                    )
                    
                    if result:
                        all_results.append(result)
                        print(f"    ‚úÖ Experiment completed")
                    else:
                        print(f"    ‚ùå Experiment failed")
        
        # Save results
        if all_results:
            print(f"\\nüíæ Saving {len(all_results)} experiment results...")
            filename = framework.save_results(all_results)
            
            print("\\nüìä Generating experiment report...")
            report = framework.generate_experiment_report(all_results)
            print(report)
            
            report_file = Path(framework.results_dir) / f"real_bugs_report_{filename.replace('.json', '.txt')}"
            with open(report_file, 'w') as f:
                f.write(report)
            print(f"‚úì Report saved to: {report_file}")
            
            print(f"\\nüéâ Real bug experiments completed successfully!")
            print(f"Total experiments: {len(all_results)}")
            print(f"Results saved to: {filename}")
        else:
            print("\\n‚ùå No results to save")
            
    except KeyboardInterrupt:
        print("\\n‚ö†Ô∏è Experiments interrupted by user")
    except Exception as e:
        print(f"\\n‚ùå Error during experiments: {e}")

def show_bug_distribution(bugs):
    \"\"\"Show the distribution of real bugs\"\"\"
    print("\\nüìä Real SWE-bench GUI Bug Distribution:")
    
    # Count by category
    categories = {}
    for bug in bugs:
        category = bug.get('bug_category', 'unknown')
        if category not in categories:
            categories[category] = 0
        categories[category] += 1
    
    for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        print(f"  {category}: {count} bugs")
    
    # Count by severity
    severities = {}
    for bug in bugs:
        severity = bug.get('severity', 'unknown')
        if severity not in severities:
            severities[severity] = 0
        severities[severity] += 1
    
    print("\\nSeverity Distribution:")
    for severity, count in sorted(severities.items(), key=lambda x: x[1], reverse=True):
        print(f"  {severity}: {count} bugs")
    
    # Show sample bugs
    print("\\nSample Real Bugs:")
    for i, bug in enumerate(bugs[:5]):
        title = bug.get('title', 'No title')[:60]
        category = bug.get('bug_category', 'unknown')
        severity = bug.get('severity', 'unknown')
        print(f"  {i+1}. [{category}/{severity}] {title}")

def main():
    \"\"\"Main function to run real bug experiments\"\"\"
    print("üöÄ Real SWE-bench GUI Bug Experiment Runner")
    print("=" * 50)
    
    # Check if real bugs are available
    real_bugs_file = Path("data/processed/real_swe_bench_scenarios.json")
    if not real_bugs_file.exists():
        print("‚ùå Real SWE-bench scenarios not found!")
        print("Run the update_experiment_framework.py script first.")
        return
    
    # Run experiments
    run_real_bug_experiments()

if __name__ == "__main__":
    main()
"""
    
    # Save the runner
    output_dir = Path("scripts/experiments")
    output_dir.mkdir(exist_ok=True)
    
    runner_file = output_dir / "run_real_bug_experiments.py"
    with open(runner_file, 'w') as f:
        f.write(runner_code)
    
    print(f"üöÄ Real data experiment runner created: {runner_file}")

def create_jupyter_notebook():
    """Create a Jupyter notebook for cloud processing"""
    print("üìì Creating Jupyter notebook for cloud processing...")
    
    notebook_code = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# üî¨ Real SWE-bench GUI Bug Experiments (Cloud Processing)\n",
                    "\n",
                    "This notebook runs experiments on real SWE-bench GUI bugs using cloud resources.\n",
                    "Perfect for limited local resources!\n",
                    "\n",
                    "## What This Notebook Does:\n",
                    "1. **Loads real SWE-bench GUI bugs** (not synthetic scenarios)\n",
                    "2. **Runs LLM experiments** on authentic bug reports\n",
                    "3. **Generates research results** for research documents\n",
                    "4. **Uses cloud resources** for heavy processing\n",
                    "\n",
                    "## Setup:\n",
                    "```bash\n",
                    "pip install datasets openai anthropic\n",
                    "```\n",
                    "\n",
                    "## Environment Variables:\n",
                    "```bash\n",
                    "export OPENAI_API_KEY='your_key_here'\n",
                    "export ANTHROPIC_API_KEY='your_key_here'\n",
                    "```"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Install required packages\n",
                    "!pip install datasets openai anthropic python-dotenv"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Load environment variables\n",
                    "import os\n",
                    "from dotenv import load_dotenv\n",
                    "\n",
                    "load_dotenv()\n",
                    "\n",
                    "print(f\"OpenAI API Key: {'‚úì' if os.getenv('OPENAI_API_KEY') else '‚ùå'}\")\n",
                    "print(f\"Anthropic API Key: {'‚úì' if os.getenv('ANTHROPIC_API_KEY') else '‚ùå'}\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Load real SWE-bench GUI bugs\n",
                    "from datasets import load_dataset\n",
                    "import json\n",
                    "\n",
                    "print(\"Loading SWE-bench Multimodal for GUI bugs...\")\n",
                    "\n",
                    "# Load a larger sample for comprehensive testing\n",
                    "swebench_mm = load_dataset('SWE-bench/SWE-bench_Multimodal', split='test[:200]')\n",
                    "print(f\"Loaded {len(swebench_mm)} samples\")\n",
                    "\n",
                    "# Extract GUI-related bugs\n",
                    "gui_keywords = ['button', 'form', 'ui', 'css', 'layout', 'click', 'frontend', 'react', 'vue', 'javascript']\n",
                    "gui_bugs = []\n",
                    "\n",
                    "for sample in swebench_mm:\n",
                    "    problem = sample.get('problem_statement', '').lower()\n",
                    "    if sum(1 for keyword in gui_keywords if keyword in problem) >= 2:\n",
                    "        gui_bugs.append({\n",
                    "            'id': sample.get('instance_id', 'unknown'),\n",
                    "            'repo': sample.get('repo', 'unknown'),\n",
                    "            'title': sample.get('problem_statement', '')[:100] + '...',\n",
                    "            'description': sample.get('problem_statement', ''),\n",
                    "            'has_image': bool(sample.get('image_assets'))\n",
                    "        })\n",
                    "\n",
                    "print(f\"Found {len(gui_bugs)} GUI-related bugs!\")\n",
                    "print(f\"Sample bugs: {len(gui_bugs[:3])}\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Run experiments on real bugs\n",
                    "import openai\n",
                    "import anthropic\n",
                    "import time\n",
                    "from datetime import datetime\n",
                    "\n",
                    "# Initialize clients\n",
                    "if os.getenv('OPENAI_API_KEY'):\n",
                    "    openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
                    "    print(\"‚úì OpenAI client initialized\")\n",
                    "\n",
                    "if os.getenv('ANTHROPIC_API_KEY'):\n",
                    "    anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
                    "    print(\"‚úì Anthropic client initialized\")\n",
                    "\n",
                    "# Test prompt for bug fixing\n",
                    "test_prompt = \"\"\"\n",
                    "You are a senior frontend engineer. Given this bug report:\n",
                    "\n",
                    "BUG REPORT:\n",
                    "{bug_description}\n",
                    "\n",
                    "REPOSITORY: {repo}\n",
                    "\n",
                    "Provide a solution in this format:\n",
                    "ROOT CAUSE: [brief explanation]\n",
                    "SOLUTION: [detailed fix steps]\n",
                    "EXPLANATION: [how this fixes the issue]\n",
                    "TESTING: [steps to verify the fix]\n",
                    "\"\"\"\n",
                    "\n",
                    "print(\"\\nüöÄ Starting experiments on real bugs...\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Run experiments and collect results\n",
                    "results = []\n",
                    "\n",
                    "# Test on first 10 bugs to start\n",
                    "test_bugs = gui_bugs[:10]\n",
                    "\n",
                    "for i, bug in enumerate(test_bugs):\n",
                    "    print(f\"\\nüîç Testing bug {i+1}/{len(test_bugs)}: {bug['title'][:60]}...\")\n",
                    "    \n",
                    "    # Format prompt\n",
                    "    prompt = test_prompt.format(\n",
                    "        bug_description=bug['description'][:1000],\n",
                    "        repo=bug['repo']\n",
                    "    )\n",
                    "    \n",
                    "    try:\n",
                    "        # Test with OpenAI if available\n",
                    "        if 'openai_client' in locals():\n",
                    "            start_time = time.time()\n",
                    "            response = openai_client.chat.completions.create(\n",
                    "                model=\"gpt-4o-mini\",\n",
                    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                    "                temperature=0.1\n",
                    "            )\n",
                    "            processing_time = time.time() - start_time\n",
                    "            \n",
                    "            results.append({\n",
                    "                'bug_id': bug['id'],\n",
                    "                'repo': bug['repo'],\n",
                    "                'model': 'gpt-4o-mini',\n",
                    "                'response': response.choices[0].message.content,\n",
                    "                'processing_time': processing_time,\n",
                    "                'timestamp': datetime.now().isoformat()\n",
                    "            })\n",
                    "            \n",
                    "            print(f\"  ‚úÖ OpenAI experiment completed in {processing_time:.2f}s\")\n",
                    "        \n",
                    "        # Test with Anthropic if available\n",
                    "        if 'anthropic_client' in locals():\n",
                    "            start_time = time.time()\n",
                    "            response = anthropic_client.messages.create(\n",
                    "                model=\"claude-3-5-sonnet-20241022\",\n",
                    "                max_tokens=1000,\n",
                    "                temperature=0.1,\n",
                    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                    "            )\n",
                    "            processing_time = time.time() - start_time\n",
                    "            \n",
                    "            results.append({\n",
                    "                'bug_id': bug['id'],\n",
                    "                'repo': bug['repo'],\n",
                    "                'model': 'claude-3-5-sonnet',\n",
                    "                'response': response.content[0].text,\n",
                    "                'processing_time': processing_time,\n",
                    "                'timestamp': datetime.now().isoformat()\n",
                    "            })\n",
                    "            \n",
                    "            print(f\"  ‚úÖ Anthropic experiment completed in {processing_time:.2f}s\")\n",
                    "    \n",
                    "    except Exception as e:\n",
                    "        print(f\"  ‚ùå Experiment failed: {e}\")\n",
                    "    \n",
                    "    # Small delay between experiments\n",
                    "    time.sleep(1)\n",
                    "\n",
                    "print(f\"\\nüéâ Completed {len(results)} experiments!\")\n",
                    "print(f\"Results: {len(results)} successful experiments\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Save results and generate report\n",
                    "if results:\n",
                    "    # Save results\n",
                    "    results_file = f\"real_bugs_experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
                    "    with open(results_file, 'w') as f:\n",
                    "        json.dump(results, f, indent=2)\n",
                    "    print(f\"üíæ Results saved to: {results_file}\")\n",
                    "    \n",
                    "    # Generate summary\n",
                    "    print(\"\\nüìä Experiment Summary:\")\n",
                    "    print(f\"Total experiments: {len(results)}\")\n",
                    "    \n",
                    "    # Count by model\n",
                    "    models = {}\n",
                    "    for result in results:\n",
                    "        model = result['model']\n",
                    "        if model not in models:\n",
                    "            models[model] = 0\n",
                    "        models[model] += 1\n",
                    "    \n",
                    "    for model, count in models.items():\n",
                    "        print(f\"  {model}: {count} experiments\")\n",
                    "    \n",
                    "    # Average processing time\n",
                    "    avg_time = sum(r['processing_time'] for r in results) / len(results)\n",
                    "    print(f\"\\nAverage processing time: {avg_time:.2f} seconds\")\n",
                    "    \n",
                    "    # Show sample response\n",
                    "    if results:\n",
                    "        print(f\"\\nüìù Sample Response (first 200 chars):\")\n",
                    "        sample = results[0]['response'][:200]\n",
                    "        print(sample + \"...\")\n",
                    "    \n",
                    "    print(\"\\nüéØ Research now has authentic results from real SWE-bench bugs!\")\n",
                    "else:\n",
                    "    print(\"\\n‚ùå No results to save\")"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Save the notebook
    output_dir = Path("notebooks")
    output_dir.mkdir(exist_ok=True)
    
    notebook_file = output_dir / "real_swe_bench_experiments.ipynb"
    with open(notebook_file, 'w') as f:
        json.dump(notebook_code, f, indent=2)
    
    print(f"üìì Jupyter notebook created: {notebook_file}")

def main():
    """Main function to update the experiment framework"""
    print("üîÑ Updating Experiment Framework to Use Real SWE-bench Bugs")
    print("=" * 60)
    
    # Backup original scenarios
    backup_original_scenarios()
    
    # Update the framework
    if update_experiment_framework():
        print("\n‚úÖ Successfully updated experiment framework!")
        
        # Create additional tools
        create_real_data_experiment_runner()
        create_jupyter_notebook()
        
        print("\nüéâ Framework Update Complete!")
        print("\nüìã What was accomplished:")
        print("‚úÖ Backed up original synthetic scenarios")
        print("‚úÖ Updated experiment framework to use real SWE-bench bugs")
        print("‚úÖ Created real data experiment runner")
        print("‚úÖ Created Jupyter notebook for cloud processing")
        
        print("\nüöÄ Next Steps:")
        print("1. Run experiments on real bugs: python scripts/experiments/run_real_bug_experiments.py")
        print("2. Use Jupyter notebook for cloud processing: notebooks/real_swe_bench_experiments.ipynb")
        print("3. Generate authentic research results for research documents!")
        
        print(f"\nüí° Research now uses 46 real GUI bugs instead of synthetic scenarios!")
print("This significantly improves the credibility and validity of research!")
        
    else:
        print("\n‚ùå Framework update failed. Check the error messages above.")

if __name__ == "__main__":
    main()
